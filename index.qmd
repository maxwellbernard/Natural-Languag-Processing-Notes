---
title: "NLP Class Notes"
author: "Maxwell Bernard"
output-dir: docs
format:
    html:
        toc: true
        toc-depth: 3
        toc-expand: true
        css: styles.css
        code-block-line-numbers: true
        code-block-wrap: true
        code-overflow: wrap
        code-output-overflow: wrap
        theme: default
        code-block-theme: default
        highlight-style: pygments
        self-contained: true
---

# Lecture 2

```{python}
# | echo: false
# | label: Library imports
# | output: false
from nltk.corpus import stopwords  # for removing stopwords
from nltk.tokenize import word_tokenize, sent_tokenize, TreebankWordTokenizer  # for tokenization
import spacy  # for spaCy library
from spacy import displacy  # for visualizing named entities
import nltk
from nltk import FreqDist, ne_chunk

# nltk.download("stopwords")  # download stopwords to local machine
from urllib import request  # for fetching data from URLs
import string  # for string operations
from nltk.stem import WordNetLemmatizer, SnowballStemmer  # for lemmatization and stemming
from nltk.tag import pos_tag  # for POS tagging
from nltk.tree import Tree  # for representing syntactic structure
from prettytable import PrettyTable  # for generating ASCII tables
from nltk.util import ngrams  # ngrams is a function that returns the n-grams of a text
from collections import Counter  # Counter is a dict subclass for counting hashable objects
from nltk.lm import MLE  # Maximum Likelihood Estimation
import re  # Regular Expressions
```

## Libraries

|  | NLTK | spaCy |
|---------|---------|---------|
| Methods | string processing library | object-oriented approach - it parses a text, returns document object whose words and sentences are objects themselves | 
| Tokenization | uses regular expression-based methods which are not always accurate | uses a rule-based approach to tokenization which is more accurate |
| POS Tagging | provides wide range of POS taggers (ranging from rule-based to machine learning-based) | uses a deep learning-based POS tagger which is more accurate (also offers pre-trained models in multiple languages) |
| Named Entity Recognition (NER) | provides multiple NER taggers (ranging from rule-based to machine learning-based) | uses a highly efficient deep learning-based NER tagger for detecting entities such as names, organizations, locations, etc. |
| Performance | slower than spaCy | faster than NLTK (due to its optimized implementation in Cython) |

- `Textblob` is another popular library for NLP in Python. It is built on top of NLTK and provides a simple API for common NLP tasks such as tokenization, POS tagging, and sentiment analysis. 
  - It is **not good for large scale production use**

## Normalization

Normalization is the process of converting a text into a standard form. This involves removing any characters that are not part of the standard English alphabet, converting all characters to lowercase, and removing any extra spaces.

Tasks involved in normalization include

- **Tokenization**: Breaking a text into words, phrases, symbols, or other meaningful elements.
- **Case Folding**: Converting all characters to lowercase.
- **Removing Punctuation**: Removing any non-alphanumeric characters.
- **Removing Stopwords**: Removing common words that do not carry much meaning.
- **Stemming or Lemmatization**: Reducing words to their base or root form.
- **Removing Extra Spaces**: Removing any extra spaces between words.
- **Expanding Contractions**: Expanding contractions such as "don't" to "do not".
- **Removing URLs and Emails**: Removing URLs, email addresses, and other web-related content.
- **Removing HTML Tags**: Removing HTML tags from web pages.
- **Removing Emojis and Special Characters**: Removing emojis, emoticons, and other special characters.

**Removal of Stopwords in Python using NLTK**
```{python}
text = "NLTK and spaCy are popular NLP libraries used for text processing."
tokens = word_tokenize(text)
stop_words = set(stopwords.words("english"))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
print(f"NLTK stopwords removal: {' '.join(filtered_tokens)}")
```

**Removal of Stopwords in Python using spaCy**
```{python}
nlp = spacy.load("en_core_web_sm") # Load English language model
text = "NLTK and spaCy are popular NLP libraries used for text processing."
doc = nlp(text)
filtered_tokens = [token.text for token in doc if not token.is_stop]
print(f"spaCy stopwords removal: {' '.join(filtered_tokens)}")
```

#### Morphological Normalization

- Morphological normalization is the process of reducing a word to its base or root form
- This can involve **stemming** or **lemmatization**
- **Roots** are the base forms of words eg. "run" is the root of "running", "ran", "runs"
- **Affixes** are the prefixes and suffixes that are added to roots to create different forms of words eg. "ing", "ed", "s"
  - **Prefixes** are added to the beginning of a word eg. "un" in "undo"
  - **Suffixes** are added to the end of a word eg. "ly" in "quickly"
- **Inflectional** affixes are added to a word to change its grammatical form eg. "s" for plural nouns, "ed" for past tense verbs eg. "dogs", "walked"

### Tokenization

- Tokenization is the process of breaking a text into words, phrases, symbols, or other meaningful elements.
- The tokens are the words, sentences, characters, or subwords that are produced by the tokenization process.
- Space based token is used to prepare the text for further processing such as stemming, lemmatization, and POS tagging.
- **Type** is number of unique tokens in a text
- **Token** is a single instance of a token in a text

```{python}
# | eval: false
tokens = word_tokenize(text)
total_tokens = len(tokens)
total_types = len(set(tokens))
```
- **Corpus** is a collection of text documents

##### Tokenization of raw text in Python
- word.isalnum() - returns True if all characters in the word are alphanumeric

```{python}
#| eval: false
shakespeare_url = "https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt"
crime_punishment_url = "http://www.gutenberg.org/files/2554/2554-0.txt"

# Tokenize Raw Text

shakespeare_text = request.urlopen(shakespeare_url).read().decode("utf8")
crime_punishment_text = request.urlopen(crime_punishment_url).read().decode("utf8")

shakespeare_tokenized = [word for word in word_tokenize(shakespeare_text.lower()) if word.isalnum()]
crime_punishment_tokenized = [
    word for word in word_tokenize(crime_punishment_text.lower()) if word.isalnum()
]

# FreqDist (from nltk) to produce a frequency distribution (listing top 20 most common words)

shakespeare_freq = FreqDist(shakespeare_tokenized)
crime_freq = FreqDist(crime_punishment_tokenized)

shakespeare_common = shakespeare_freq.most_common(20)
crime_common = crime_freq.most_common(20)

print("Top 20 words in Shakespeare")
print(shakespeare_common)

print("\nTop 20 words in Crime and Punishment:")
print(crime_common)
```

##### NLTK Tokenizers
- `word_tokenize()`: Tokenizes a text into words using a regular expression-based tokenizer. A versatile tokenizer that handles contractions, abbreviations, and punctuation marks effectively. It is suitable for most general-purpose tokenization tasks.
- `WordPunctTokenizer()`: Tokenizes a text into words using a regular expression that matches punctuation as separate tokens. A specialized tokenizer that separates words and punctuation explicitly. It is useful when you need to distinguish between alphabetic and non-alphabetic characters.
  
### Stemming

- Stemming is the process of reducing a word to its root or base form. For example, the word "running" would be reduced to "run" by a stemming algorithm.
- Stemmers remove word suffixes by running input word tokens against a pre-defined list of common suffixes.
- Stemming is a heuristic process that does not always produce a valid root word, but it can be useful for text processing tasks such as search indexing and information retrieval.
- **Porter Stemmer** is a popular (rule-based) stemming algorithm that is widely used in text processing applications.
- **Snowball Stemmer** is an improved version of the Porter Stemmer that supports multiple languages.

#### Example of stemming in Python using NLTK

```{python}
sbs = SnowballStemmer("english")
text="The stars twinkled in the dark, illuminating the night sky."
method = TreebankWordTokenizer()
word_tokens = method.tokenize(text)
stemmed = [sbs.stem(token) for token in word_tokens]

for i in range(len(word_tokens)):
    print(f"{word_tokens[i]} | {stemmed[i]}")
```

### Lemmatization

- The process of reducing a word to its base or root form, known as a lemma.
- Is more sophisticated than stemming because it uses a dictionary to map words to their base forms.
- POS tagging is used to determine the correct lemma for a word based on its part of speech.
- Lemmatization is more accurate than stemming but can be slower and more computationally intensive.
- **WordNet Lemmatizer** is a popular lemmatization algorithm that is available in NLTK.

#### Example of stemming in Python using NLTK
```{python}
sbs = SnowballStemmer("english")
text="The stars twinkled in the dark, illuminating the night sky."
method = TreebankWordTokenizer()
word_tokens = method.tokenize(text)
lemma = WordNetLemmatizer()
lemmatized = [lemma.lemmatize(token) for token in word_tokens]

for i in range(len(word_tokens)):
    print(f"{word_tokens[i]} | {lemmatized[i]}")
```


**Comparison table of stemming and lemmatization**

| Stemming | Lemmatization |
|----------|---------------|
| Faster | Slower (Resource Intensive) |
| Less accurate | More accurate |
| Uses heuristics (eg choppping off endings) | Uses a dictionary-based lookup |
| Produces root words | Produces base words |
| Removes word suffixes | Maps words to base forms |
| Does not require POS tagging | Requires POS tagging |


## Regular Expressions

- Regular expressions are a powerful tool for **pattern matching** and text processing.
- They allow you to search for patterns in text, extract specific information, and perform complex text transformations.

**Regular Expression (Disjunction) Table**

| Pattern | Matches | Example |
|---------|---------|---------|
| . | Any character except newline | "a.b" matches "axb", "a1b", "a\@b" |
| ^ | Start of string | "^abc" matches "abc", "abcd", "abc123" |
| \$ | End of string | "abc\$" matches "abc", "123abc", "xyzabc" |
| \* | Zero or more occurrences | "ab*c" matches "ac", "abc", "abbc" |
| \+ | One or more occurrences | "ab+c" matches "abc", "abbc", "abbbc" |
| ? | Zero or one occurrence | "ab?c" matches "ac", "abc" |
| {n} | Exactly n occurrences | "ab{2}c" matches "abbc" |
| {n,} | n or more occurrences | "ab{2,}c" matches "abbc", "abbbc" |
| {n,m} | Between n and m occurrences | "ab{2,3}c" matches "abbc", "abbbc" |
| [abc] | Any character in the set | "[abc]" matches "a", "b", "c" |
| [^abc] | Any character not in the set | "[^abc]" matches "d", "e", "f" |
| [A-Z] | Any character in the range | "[A-Z]" matches "A", "B", "C" |
| [a-z] | Any character in the range | "[a-z]" matches "a", "b", "c" |
| [0-9] | Any digit | "[0-9]" matches "0", "1", "2" |
| \\d | Any digit | "\\d" matches "0", "1", "2" |
| \\D | Any non-digit | "\\D" matches "a", "b", "c" |
| \\w | Any word character | "\\w" matches "a", "b", "c", "0", "1", "2" |
| \\W | Any non-word character | "\\W" matches "@", "#", "$" |
| \\s | Any whitespace character | "\\s" matches " ", "\t", "\n" |
| \\S | Any non-whitespace character | "\\S" matches "a", "b", "c" |

**Regular Expression Method Table**

| Method | Description |
|--------|-------------|
| re.match() | Matches a pattern at the beginning of a string |
| re.search() | Searches for a pattern in a string |
| re.findall() | Finds all occurrences of a pattern in a string |
| re.split() | Splits a string based on a pattern |
| re.sub() | Replaces a pattern in a string with another pattern |


**Example of Regular Expressions in Python**

```{python}
text = "The quick brown fox jumps over the lazy dog. The dog barks at the fox."
sentences = sent_tokenize(text)
words = word_tokenize(text)

# Find all words that start with "b"
pattern = r"\b[bB]\w+"  # \b is a word boundary, \w+ is one or more word characters
for word in words:
    if re.match(pattern, word):
        print(f" All words that start with 'b': {word}")

# Find all sentences that contain the word "dog"
pattern = r"\b[dD]ogs?\b"  # \b is a word boundary, ? is zero or one occurrence of previous character, s? matches "dog" or "dogs"
for sentence in sentences:
    if re.search(pattern, sentence):
        print(f" All sentences containing word 'dog': {sentence}")
```

## POS Tagging

- Part-of-speech (POS) tagging is the process of assigning a part of speech to each word in a text. The part of speech indicates the grammatical category of the word, such as noun, verb, adjective, etc.
- The goal of POS tagging is to identify the syntactic structure of a sentence and extract useful information about the text.

- **Types of POS Tagging**
  - **Rule-based POS Tagging**:
    - Uses hand-crafted rules to assign POS tags to words based on their context.
    - Relies on a predefined set of rules and patterns to determine the correct POS tag for a word.
    - **Pros**:
        - Simple and easy to implement, but may not be as accurate as other methods.
        - Doesnt require a lot of computational resources or training data.
        - Can be easily customized and adapted to different languages and domains.
      - **Cons**:
        - May not be as accurate as other methods, especially for complex or ambiguous cases.
        - Requires manual effort to define rules and patterns for different languages and domains.
        - Limited to the rules and patterns defined by the developer, which may not cover all cases.
      - **Example**: Rule-based POS taggers in NLTK such as the `DefaultTagger` and `RegexpTagger` and `pos_tag` method.
<br>
<br>

  - **Statistical POS Tagging**:
  - Uses statistical models (such as Hidden Markov Models) to assign POS tags to words based on their context and probability.
  - Learns the patterns and relationships between words and their POS tags from a large corpus of annotated text.
    - **Pros**:
      - More accurate than rule-based methods, especially for complex or ambiguous cases.
      - Can handle a wide range of languages and domains without manual intervention.
      - Can be trained on large datasets to improve accuracy and performance.
    - **Cons**:
      - Requires a large amount of annotated training data to train the statistical model.
      - Can be computationally intensive and require significant resources for training and inference.
      - May not be as interpretable or transparent as rule-based methods, making it difficult to understand the model's decisions.
    - **Examples**: Machine learning-based POS taggers in spaCy such as the `PerceptronTagger` and `CNNTagger`.

**Most Common POS Tags (NLTK)**
  
| Tag | Description | Example |
|-----|-------------|---------|
| CC | Coordinating conjunction | and, but, or |
| CD | Cardinal number | 1, 2, 3 |
| DT | Determiner | the, a, an |
| EX | Existential there | there |
| FW | Foreign word | bonjour |
| IN | Preposition or subordinating conjunction | in, of, on |
| JJ | Adjective | big, green, happy |
| JJR | Adjective, comparative | bigger, greener, happier |
| JJS | Adjective, superlative | biggest, greenest, happiest |
| LS | List item marker | 1, 2, 3 |
| MD | Modal | can, could, might |
| NN | Noun, singular or mass | dog, cat, house |
      
<br>
<br>

**Most Common POS Tags (SpaCy)**

| Tag | Description | Example |
|-----|-------------|---------|
| ADJ | Adjective | big, green, happy |
| ADP | Adposition | in, to, during |
| ADV | Adverb | very, tomorrow, down |
| AUX | Auxiliary | is, has (done), will |
| CONJ | Conjunction | and, or, but |
| CCONJ | Coordinating conjunction | and, or, but |
| DET | Determiner | a, an, the |
| INTJ | Interjection | psst, ouch, bravo |
| NOUN | N
| NUM | Numeral | 42, forty-two |

#### Example of POS Tagging in Python using NLTK
```{python}
text = "The quick brown fox jumps over the lazy dog."
words = word_tokenize(text)
tags = pos_tag(words)

for word, tag in tags:
    print(f"{word} | {tag}")
```

#### Example of POS Tagging in Python using spaCy
- Make sure to download the spaCy model using `python -m spacy download en_core_web_sm`
- This model is a small English model trained on written web text (blogs, news, comments), which includes vocabulary, vectors, syntax, and entities.
- The model is trained on the OntoNotes 5 corpus and supports POS tagging, dependency parsing, named entity recognition, and more.
```{python}
model = spacy.load("en_core_web_sm")
sample_text = "The quick brown fox jumps over the lazy dog."
doc = model(sample_text)

for word in doc:
    print(f"{word.text} | {word.pos_}")
```

## Named Entity Recognition (NER)
- Named Entity Recognition (NER) is the process of identifying and classifying named entities in a text.
- Named entities are real-world objects such as persons, organizations, locations, dates, and more.
- NER is an important task in NLP because it helps extract useful information from unstructured text and enables downstream tasks such as information retrieval, question answering, and text summarization.
- NER models are trained on annotated datasets that contain labeled examples of named entities in text.
- **Common types of named entities**:
  - **Person**: Names of people, such as "John Doe" or "Alice Smith".
  - **Organization**: Names of companies, institutions, or groups, such as "Google" or "United Nations".
  - **Location**: Names of places, such as "New York" or "Mount Everest".
  - **Date**: Dates and times, such as "January 1, 2022" or "10:30 AM".
  - **Number**: Numerical quantities, such as "100" or "3.14".
  - **Miscellaneous**: Other named entities, such as "Apple" (the company) or "Python" (the programming language).
  - **Event**: Names of events, such as "World War II" or "Super Bowl".
  - **Product**: Names of products, such as "iPhone" or "Coca-Cola".

#### Example of Named Entity Recognition in Python using NLTK
In NLP, a tree structure is often used to represent the syntactic structure of a sentence. Each node in the tree represents a linguistic unit, such as a word or a phrase, and the edges represent the relationships between these units. The `Tree` class provides various methods for creating, traversing, and modifying these tree structures.
```{python}
text = "Apple is a technology company based in Cupertino, California."
words = word_tokenize(text)
tags = pos_tag(words)
tree = ne_chunk(tags)

for subtree in tree:
    if isinstance(subtree, Tree):
        label = subtree.label()
        words = " ".join([word for word, tag in subtree.leaves()])
        print(f"{label}: {words}")
```
- Note: NLTK's named entity recognizer has identified "Apple", "Cupertino", and "California" as geopolitical entities (GPE).
- The NER model uses context and patterns learned from training data to classify named entities, but it is not always perfect and can sometimes make mistakes, as seen with "Apple" in this case.

#### Example of Named Entity Recognition in Python using spaCy
```{python}
model = spacy.load("en_core_web_sm")
sample_text = "Apple is a technology company based in Cupertino, California."
doc = model(sample_text)

displacy.render(doc, style="ent", jupyter=True) # style="ent" is used to display named entities
```


# Lecture 3

## N-Gram (Probabilistic LM)
This is model predicts the probability of a word given the previous n-1 words. It is based on the assumption that the probability of a word depends on the context provided by the previous n-1 words. N-grams are used in various NLP tasks such as speech recognition, machine translation, and text generation.

- **Unigram**: A single word
- **Bigram**: A pair of words
- **Trigram**: A triplet of words ect...

number of n-grams = (total number of words) - (n - 1)

**Generating different n-grams for a text**:

```{python}
text = "Italy is famous for its cuisine."

def generate_ngrams(text, n):
    tokens = word_tokenize(text)
    ngrams_list = list(ngrams(tokens, n))
    return [" ".join(ngram) for ngram in ngrams_list]

trigrams = generate_ngrams(text, 3)
```

### Conditional Probability and Chain Rule

- **Joint Probability**: The probability of two or more events occurring together. It is denoted as P(A, B) and is calculated as the probability of both events A and B occurring.
  - **Joint Probability Formula**: $P(A \cap B) = P(A) \times P(B)$
  - is equivalent to $P(A \cap B) = P(A) \times P(B)$
<br>
<br>

- **Conditional Probability**: The probability of an event given that another event has occurred. It is denoted as P(A | B) which is the probability of event A occurring given that event B has occurred.
- It is calculated as the probability of both events A and B occurring divided by the probability of event B occurring.
  - **Conditional Probability Formula**: $P(A \mid B) = \frac{P(A, B)}{P(B)}$
<br>
<br>

- **Chain Rule of Probability**: The probability of a sequence of events can be calculated by multiplying the conditional probabilities of each event given the previous events in the sequence.
  - **Chain Rule Formula**:
$$
\small
P(w_1, w_2, ..., w_n) = P(w_1) * P(w_2 \mid w_1) * P(w_3 \mid w_1, w_2) * ... * P(w_n \mid w_1, w_2, ..., w_{n-1})
$$

  - **Example**: The probability of the sequence "The quick brown fox" can be calculated using the chain rule as follows:
$$
\small
\begin{aligned}
P(\text{The quick brown fox}) &= P(\text{The}) \times P(\text{quick} \mid \text{The}) \\
&\times P(\text{brown} \mid \text{The quick}) \\
&\times P(\text{fox} \mid \text{The quick brown})
\end{aligned}
$$
    - "What is the probability of the 10th word given the previous 9 words?"
  - **Note**: The chain rule is used to calculate the probability of a sequence of words in an N-gram model.

### Markov Assumption
To make things manageable, N-gram models approximate the chain rule by making a simplifying assumption called the Markov assumption. This assumption states that the **probability of a word depends only on the previous n-1 words, not on all the previous words in the sequence.** This simplifies the calculation of the conditional probability of a word given the previous n-1 words.


- **First-Order Markov Model**: The probability of a word depends only on the previous word.
  - $$P(w_n | w_1, w_2, ..., w_{n-1}) = P(w_n | w_{n-1})$$
  - "The probability of the 10th word depends only on the 9th word, not on the previous words."
  - **Example**: $P(\text{fox} \mid \text{The quick brown}) = P(\text{fox} \mid \text{brown})$
  - **Note**: The first-order Markov model is a special case of the N-gram model where n=2.
<br>
<br>

- **Second-Order Markov Model**: The probability of a word depends only on the previous two words.
  - $$P(w_n | w_1, w_2, ..., w_{n-1}) = P(w_n | w_{n-2}, w_{n-1})$$
  - "The probability of the 10th word depends only on the 8th and 9th words, not on the previous words."
  - **Example**: $P(\text{fox} \mid \text{The quick brown}) = P(\text{fox} \mid \text{quick brown})$
  - **Note**: The second-order Markov model is a special case of the N-gram model where n=3.

**N-Gram Probability Calculation**

- Mathematically, for a sequence of words $w_1, w_2, ..., w_n$, the probability of the sequence can be calculated using the chain rule of probability:
  - $$\small P(w_1, w_2, ..., w_n) = P(w_1) * P(w_2 \mid w_1) * P(w_3 \mid w_1, w_2) * ... * P(w_n \mid w_1, w_2, ..., w_{n-1})$$

- The probability of a word given the previous n-1 words can be calculated using the formula:
  - $P(w_n | w_1, w_2, ..., w_{n-1}) = \frac{P(w_1, w_2, ..., w_n)}{P(w_1, w_2, ..., w_{n-1})}$
<br>

- The probability of a sequence of words can be calculated by counting the occurrences of the n-gram in a corpus and dividing by the total number of n-grams in the corpus.
<br>

- **The Maximum Likelihood Estimation (MLE) of an N-gram** is calculated as:
  - $P(w_n | w_1, w_2, ..., w_{n-1}) = \frac{\text{Count(n-gram)}}{\text{Count(of previous n-1 words)}}$
<br>
<br>
  - Issue: The MLE can assign zero probability to unseen n-grams, leading to sparsity and poor generalization.
  
### Padding 
Is a technique used to handle the boundary conditions in N-gram models where the context words are not available.
- **Start-of-Sentence (BOS) Padding**: A special token that represents the beginning of a sentence. It is used to handle the first word in a sentence where the context words are not available.
  - **Example**: `<s>` The quick brown fox jumps over the lazy dog. Note `<s>` is equivalent to `<start>`
<br>
<br>

- **End-of-Sentence (EOS) Padding**: A special token that represents the end of a sentence. It is used to handle the last word in a sentence where the context words are not available.
  - **Example**: The fox jumps over the lazy dog. `</s>`

**Notes**: the choice of how many padding tokens to use depends on the order of the N-gram model. For a bigram model, you would use one padding token at the beginning of the sentence. For a trigram model, you would use two padding tokens at the beginning of the sentence. The padding tokens are not part of the vocabulary and are used only for modeling purposes.

**Padding Example in Python**

```{python}
text = "I like NLP. NLP is fun. NLP in python is fun."
sentences = sent_tokenize(text)

# Pad each sentence individually with <s> and </s> tokens and tokenize into words
padded_sentences = []
for sentence in sentences:
    words = word_tokenize(sentence)
    padded = ["<s>"] + words + ["</s>"]
    padded_sentences.append(padded)

for sentence in padded_sentences:
    print(sentence)
```


### Issue of Underflow
- **Underflow**: A numerical issue that occurs when multiplying many small probabilities together, leading to a very small probability that may be rounded to zero.
- **Solution**: Use ``log probabilities`` to avoid underflow by converting the product of probabilities to the sum of log probabilities.

### Smoothing Techniques
- This is a technique used to address the issue of zero probabilities in N-gram models. It assigns a small non-zero probability to unseen n-grams to improve the generalization of the model. You "test" these by comparing smoothed probabilities to unsmoothed ones.
<br>
<br>

- **Laplace (Add-One) Smoothing**: A simple smoothing technique that adds one to the count of each n-gram to avoid zero probabilities.
  - **Formula**: $P(w_n | w_1, w_2, ..., w_{n-1}) = \frac{(\text{Count(n-gram)} + 1)}{(\text{Count(of previous n-1 words)} + V)}$
    - **V**: The vocabulary size, which is the total number of unique words in the corpus.
    - **Example**: $$P(\text{fox} \mid \text{quick brown}) = \frac{\text{Count}(\text{quick brown fox}) + 1}{\text{Count}(\text{quick brown}) + V}$$

- **Add-k Smoothing**: A generalization of Laplace smoothing that adds a `smaller constant k` to the count of each n-gram to avoid zero probabilities.
  - **Formula**: $P(w_n | w_1, w_2, ..., w_{n-1}) = \frac{(\text{Count(n-gram)} + k)}{(\text{Count(of previous n-1 words)} + kV)}$
    - **k**: A constant value that determines the amount of smoothing.
    - **Example**: $$P(\text{fox} \mid \text{quick brown}) = \frac{\text{Count}(\text{quick brown fox}) + k}{\text{Count}(\text{quick brown}) + kV}$$
    - **Test**: Tune k and compare probabilities for rare vs. frequent trigrams.

- **Good-Turing Smoothing**: A more sophisticated smoothing technique that estimates the probability of unseen n-grams based on the frequency of seen n-grams.
  - adjusts the counts of n-grams based on the frequency of n-grams with similar counts.
  - **Test**: compute adjusted probabilities for low-frequency n-grams and validate against held-out data.

- **Backoff and Interpolation**: A technique that combines n-gram models of different orders to improve the generalization of the model.
  - **Backoff**: Uses lower-order n-grams when higher-order n-grams have zero probabilities.
  - **Interpolation**: Combines probabilities from different n-gram models using linear interpolation.
  - **Test**: Compare performance of backoff and interpolation on different datasets.

- **Kneser-Ney Smoothing**: A state-of-the-art smoothing technique that estimates the probability of unseen n-grams based on the frequency of n-grams in the context of the n-gram.
  - It considers how often a word appears in a novel context, rather than just how often it appears overall.
  - **Test**: Compare Kneser-Ney smoothing to other smoothing techniques on large datasets.


### Perplexity
- **Perplexity**: A measure of how well a language model predicts a given text. It is the inverse probability of the test set, normalized by the number of words.
- Evaluation Metric: "How well the model predicts the next word in a sequence."

  - **Perplexity Formula**: $PP(W) = P(w_1, w_2, ..., w_N)^{-\frac{1}{N}}$
    - which can be calculated as $PP(W) = \left(\frac{1}{P(w_1, w_2, ..., w_N)}\right)^{\frac{1}{N}}$
    - where:
      - **P(w_1, w_2, ..., w_N)**: Probability of the test set under the language model.
      - **N**: Number of words in the test set.
      - **Example**: If the perplexity of a language model is 100, it means that the model is as confused as if it had to choose uniformly among 100 words for each word in the test set.
    - **N**: The number of words in the test set.
    - **Example**: If the perplexity of a language model is 100, it means that the model is as confused as if it had to choose uniformly among 100 words for each word in the test set.
  - <span style="color:green">Lower </span>Perplexity: <span style="color:green">better</span> language model that predicts the test set more accurately.
  - <span style="color:red">Higher </span>Perplexity: <span style="color:red">worse</span> language model that predicts the test set less accurately.

### Example of Trigram LM in Python
- Steps:
  - Tokenize text
  - Add padding tokens
  - Generate trigrams
  - Count unique trigrams
  - Calculate trigram probabilities (MLE)
  - Calculate perplexity

**STEP 1: tokenize text and add padding tokens**
```{python}
text = "I like NLP. NLP is fun. NLP in python is fun. I like coding in python. NLP is cool."
tokens = sent_tokenize(text)
padded_sentences = []
for token in tokens:
    words = word_tokenize(token)
    padded = ["<s>"] + words + ["</s>"]
    padded_sentences.append(padded)

print("Padded Sentences:")
for sent in padded_sentences:
    print(sent)
```

**STEP 2: Generate trigrams (and bigrams for MLE calculation)**
```{python}
trigrams = []
for sent in padded_sentences:
    sent_trigrams = list(ngrams(sent, 3))
    # sent_trigrams = list(ngrams(sent, 3, pad_left=False, pad_right=False, left_pad_symbol="<s>", right_pad_symbol="</s>"))
    # can use the above line to avoid padding tokens in trigrams, but its less flexible if you need to reuse the padded data.
    trigrams.extend(sent_trigrams)

bigrams = []
for sent in padded_sentences:
    sent_bigrams = list(ngrams(sent, 2))
    bigrams.extend(sent_bigrams)

print("\nTrigrams:")
for trigram in trigrams:
    print(trigram)
```

**STEP 3: Count unique trigrams and bigrams**
```{python}
trigram_counts = Counter(trigrams)
bigrams_counts = Counter(bigrams)
unique_bigrams = len(bigrams_counts)
unique_trigrams = len(trigram_counts)
print("\nUnique Trigrams:", unique_trigrams)
print("Unique Bigrams:", unique_bigrams)

c_tri_tab = PrettyTable(["Index", "Unique Trigram", "Count"])
for i, (trigram, count) in enumerate(trigram_counts.items()):
    c_tri_tab.add_row([i, trigram, count])
print(c_tri_tab)
```

**STEP 4: Calculate trigram probabilities (MLE)**
```{python}
tri_mle = {}
for (w1, w2, w3), count in trigram_counts.items():
    tri_mle[(w1, w2, w3)] = round(count / bigrams_counts[(w1, w2)], 3)

print("\nTrigram MLE:")
tri_mle_tab = PrettyTable(["Word 1", "Word 2", "Word 3", "MLE"])
for (w1, w2, w3), mle in tri_mle.items():
    tri_mle_tab.add_row([w1, w2, w3, mle])
print(tri_mle_tab)
```

**(Calculating MLE with Laplace Smoothing)**
```{python}
V = len(trigram_counts)
k = 1  # Laplace smoothing constant
tri_mle_laplace = {}

for (w1, w2, w3), count in trigram_counts.items():
    tri_mle_laplace[(w1, w2, w3)] = round((count + k) / (bigrams_counts[(w1, w2)] + k * V), 3)

print("\nTrigram MLE with Laplace Smoothing:")
tri_mle_laplace_tab = PrettyTable(["Word 1", "Word 2", "Word 3", "MLE (Laplace)"])
for (w1, w2, w3), mle in tri_mle_laplace.items():
    tri_mle_laplace_tab.add_row([w1, w2, w3, mle])
print(tri_mle_laplace_tab)
```

**STEP 5: Calculate Perplexity**

```{python}
import math

test_trigrams = trigrams  # Reusing the training trigrams for simplicity
# Calculate the sum of log probabilities
log_prob_sum = 0
N = len(test_trigrams)  # Number of trigrams in the test set
for trigram in test_trigrams:
    prob = tri_mle.get(trigram, 0)  # Get MLE probability, default to 0 if unseen
    if prob > 0:  # Avoid log(0)
        log_prob_sum += math.log2(prob) # use log to avoid underflow issues (A numerical issue that occurs when multiplying many small probabilities together, leading to a very small probability that may be rounded to zero)
    else:
        print(f"Warning: Trigram {trigram} has zero probability (unseen in training)")
        log_prob_sum = float("-inf")  # This will lead to infinite perplexity
        break

# Calculate perplexity
if log_prob_sum != float("-inf"):
    avg_log_prob = log_prob_sum / N
    perplexity = 2 ** (-avg_log_prob)
else:
    perplexity = float("inf")

print(f"Number of test trigrams (N): {N}")
print(f"Sum of log probabilities: {log_prob_sum:.3f}")
print(f"Average log probability: {avg_log_prob:.3f}")
print(f"Perplexity: {perplexity:.3f}")

```

### Example of Trigram LM with NLTK ABC Corpus

```{python}
print(type(list))
```
```{python}
import time
from nltk.util import trigrams, bigrams

start_time = time.time()
from nltk.corpus import abc

# Load the ABC corpus
abc_text = abc.raw("rural.txt")

# Step 1 get sentences from corpus
sentences = abc.sents()[0:2000]
print("Number of sentences:", len(sentences))

# Step 2: Tokenize text and add padding tokens
tokens = []
for sentence in sentences:
    padded_sentence = ["<s>"] + [word.lower() for word in sentence] + ["</s>"]
    tokens.extend(padded_sentence)
print("Example tokens:", tokens[:10])

# Step 3: Generate trigrams
trigram_list = list(trigrams(tokens))
bigram_list = list(bigrams(tokens))

print("Example trigrams:", trigram_list[:5])
print("Example bigrams:", bigram_list[:5])

# Step 4: Count unique trigrams
trigram_counts = Counter(trigram_list)
bigram_counts = Counter(bigram_list)
unique_trigrams = len(trigram_counts)
unique_bigrams = len(bigram_counts)
print("Unique Trigrams:", unique_trigrams)
print("Unique Bigrams:", unique_bigrams)

# Step 5: Calculate trigram probabilities (MLE) with Laplace smoothing and k=1
V = len(trigram_counts)
k = 0.01
trigram_mle_laplace = {}
for (w1, w2, w3), count in trigram_counts.items():
    trigram_mle_laplace[(w1, w2, w3)] = (count + k) / (bigram_counts[(w1, w2)] + k * V)


# Function to predict next word based on conditional probability
def predict_next_word(w1, w2, trigram_mle):
    candidates = {w3: prob for (x1, x2, w3), prob in trigram_mle.items() if x1 == w1 and x2 == w2}
    predicted = max(candidates, key=candidates.get) if candidates else None
    prob = candidates.get(predicted, 0.0) if predicted else 0.0
    return predicted, prob


# Test prediction
w1, w2 = "the", "prime"
predicted_word, probability = predict_next_word(w1, w2, trigram_mle_laplace)
print(f"Predicted next word after '{w1} {w2}': {predicted_word}")
print(f"Probability of the next word occurring: {probability:.5f}")

end_time = time.time()
print(f"Execution time: {end_time - start_time:.5f} seconds")
```

### Example of Sentence Generation with Trigram LM

```{python}
import random

start_time = time.time()


def generate_sentence(model, max_length):
    current_bigram = random.choice(list(model.keys()))  # Pick a random starting bigram
    get_text = list(current_bigram)  # Initialize with the two words from the bigram

    for _ in range(max_length - 2):  # Start from the third word
        w_next_prob = model.get(tuple(get_text[-2:]), {})  # Get trigram probabilities
        if not w_next_prob:  # If no next word, break
            break
        w_next = random.choices(list(w_next_prob.keys()), weights=list(w_next_prob.values()))[0]
        get_text.append(w_next)  # Append next word

    return " ".join(get_text)  # Return generated sentence as a string


# Example usage
generated_text = generate_sentence(trigram_mle_laplace, 15)
print(f"Generated sentence: {generated_text}")
end_time = time.time()
print(f"Execution time: {end_time - start_time:.5f} seconds")
```

# Lecture 4
## Text Classification
- Text classification is the process of assigning predefined categories or labels to text documents based on their content.
- It is a supervised learning task where a model is trained on labeled data to learn the relationship between the text and its corresponding labels.
- Text classification is used in various applications such as spam detection, sentiment analysis, topic categorization, and more.
- The goal of text classification is to build a model that can accurately predict the category of unseen text documents based on their content.
- The process of text classification involves several steps, including data preprocessing, feature extraction, model training, and evaluation.
- The choice of features and the classification algorithm can significantly impact the performance of the model.
  
There are three main types of text classification techniques:
- **Supervised Learning**: The model is trained on labeled data, where each document is associated with a predefined category. The model learns to map the input features to the corresponding labels.
  -  Naive Bayes, Logistic Regression
- **Unsupervised Learning**: The model is trained on unlabeled data, where the goal is to discover hidden patterns or clusters in the data. The model learns to group similar documents together without any predefined labels.
  - Latent Dirichlet Allocation (LDA), K-means clustering
- **Deep Learning**: The model is trained on large amounts of data using deep neural networks. Deep learning models can automatically learn complex features and representations from the data, making them suitable for text classification tasks.
  - Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Transformers (BERT, GPT-3)


## Bayes Theorem
- Bayes theorem is a fundamental concept in probability theory that describes the relationship between conditional probabilities. It is used to update the probability of a hypothesis based on new evidence.
- The equation is given by: 
$$P(H \mid E) = \frac{P(E \mid H) \cdot P(H)}{P(E)}$$
- Where:
  - P(H|E): The probability of hypothesis H given evidence E (posterior probability).
  - P(E|H): The probability of evidence E given hypothesis H (likelihood).
  - P(H): The prior probability of hypothesis H.
  - P(E): The total probability of evidence E.

- **Naive Assumption**: The naive assumption in Naive Bayes classifiers is that the features are conditionally independent given the class label. This simplifies the computation of the posterior probability.
- The naive assumption allows us to calculate the joint probability of the features given the class label as the product of the individual probabilities of each feature.
- An exmple of the naive assumption is:
- $$P(X_1, X_2, ..., X_n \mid C) = P(X_1 \mid C) \cdot P(X_2 \mid C) \cdot ... \cdot P(X_n \mid C)$$
- Where:
  - P(X1, X2, ..., Xn | C): The joint probability of features X1, X2, ..., Xn given class C.
  - P(Xi | C): The probability of feature Xi given class C.
- The naive assumption is a simplification that allows Naive Bayes classifiers to work well in practice, even when the features are not truly independent.

Types of Naive Bayes Classifiers:
- **Gaussian Naive Bayes**: Assumes that the features follow a Gaussian (normal) distribution. It is suitable for continuous features.
- **Multinomial Naive Bayes**: Assumes that the features are counts or frequencies. It is suitable for discrete features, such as word counts in text classification.
- **Bernoulli Naive Bayes**: Assumes that the features are binary (0 or 1). It is suitable for binary features, such as presence or absence of words in text classification.

Implementing Naive Bayes
- **Step 1**: Load the dataset, clean and preprocess the text data.
- **Step 2**: Split the dataset into training and testing sets.
- **Step 3**: Extract features from the text data using techniques such as Bag of Words (BoW)
- **Step 4**: Train the Naive Bayes classifier on the training set.
- **Step 5**: Evaluate the classifier on the testing set using metrics such as accuracy, precision, recall, and F1-score.

```{python}
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
from sklearn.pipeline import make_pipeline
from sklearn.datasets import fetch_20newsgroups
from nltk.corpus import stopwords
stop_words = set(stopwords.words("english"))
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
from nltk import download
from nltk import pos_tag
from nltk import ne_chunk
from nltk.tree import Tree
from nltk.tokenize import word_tokenize


# Load the dataset
newsgroups = fetch_20newsgroups(
    subset="all", categories=["alt.atheism", "sci.space"], remove=("headers", "footers", "quotes")
)
df = pd.DataFrame({"text": newsgroups.data, "label": newsgroups.target})
# Preprocess the text data


def preprocess_text(text):
    # Tokenize the text
    tokens = word_tokenize(text)
    # Remove stop words
    tokens = [word for word in tokens if word.lower() not in stop_words]
    # Stem the words
    tokens = [stemmer.stem(word) for word in tokens]
    return " ".join(tokens)


df["text"] = df["text"].apply(preprocess_text)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    df["text"], df["label"], test_size=0.2, random_state=42
)

# Create a pipeline with CountVectorizer and MultinomialNB
pipeline = make_pipeline(CountVectorizer(), MultinomialNB())

# Train the Naive Bayes classifier
pipeline.fit(X_train, y_train)

# Make predictions on the test set
y_pred = pipeline.predict(X_test)

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=newsgroups.target_names))

```

## Evaluation Metrics

- **Accuracy**: The proportion of correctly classified instances out of the total instances.
  - Formula: $Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$
  - Where:
    - TP: True Positives
    - TN: True Negatives
    - FP: False Positives
    - FN: False Negatives
- **Precision**: The proportion of true positive predictions out of all positive predictions.
  - Formula: $Precision = \frac{TP}{TP + FP}$
  - Indicates how many of the predicted positive instances are actually positive.
  - High precision means fewer false positives.
- **Recall**: The proportion of true positive predictions out of all actual positive instances.
  - Formula: $Recall = \frac{TP}{TP + FN}$
  - Indicates how many of the actual positive instances were correctly predicted.
  - High recall means fewer false negatives.
  - Also known as Sensitivity or True Positive Rate.
  - High precision and low recall means the model is conservative in making positive predictions.
- **F1-Score**: The harmonic mean of precision and recall. It is a single metric that balances both precision and recall.
  - Formula: $F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$
  - A high F1-score indicates a good balance between precision and recall.
  - Useful when the class distribution is imbalanced, because accuracy can be misleading.
  - **Classification Report**: A summary of precision, recall, and F1-score for each class in a multi-class classification problem.


Disadvatages of Naive Bayes:

  - **Independence Assumption**: The naive assumption of feature independence may not hold in real-world data, leading to suboptimal performance.
  - **Zero Probability Problem**: If a feature value is not present in the training data for a particular class, the model will assign a probability of zero to that class, which can be problematic.
  - **Probability Estimation Bias**: Naive Bayes may produce biased probability estimates, especially for rare events or classes with limited training data.
  - **Feature Revelance**: Naive Bayes may not perform well when features are highly correlated or when the feature space is large and sparse.
  - **Limited Expressiveness**: Naive Bayes is a linear classifier and may not capture complex relationships between features and classes.


If you are using a Naive Bayes classifier for a dataset
where 95% of the tweets are from positive class. Can
you suggest any method to handle this imbalance, and
briefly explain how it helps improve model
performance?

**Answer:** To handle class imbalance in a dataset where 95% of the tweets are from the positive class, you can use the following methods:

  1. **Resampling Techniques**:
      - **Oversampling**: Increase the number of instances in the minority class (negative class) by duplicating existing instances or generating synthetic samples (e.g., using SMOTE).
      - **Undersampling**: Decrease the number of instances in the majority class (positive class) by randomly removing instances.
      - **Combination**: Use a combination of oversampling and undersampling to balance the classes.
      - **How it helps**: Resampling techniques help to create a more balanced dataset, which allows the model to learn better from both classes and reduces the bias towards the majority class. This can lead to improved performance in terms of precision, recall, and F1-score for the minority class.
  2. **Class Weights**: Assign higher weights to the minority class during model training. This can be done by using the `class_weight` parameter in scikit-learn classifiers.
      - **How it helps**: By assigning higher weights to the minority class, the model is penalized more for misclassifying instances from that class, which encourages it to focus on learning from the minority class.
  3. **Ensemble Methods**: Use ensemble methods such as Random Forest or Gradient Boosting, which can handle class imbalance better than individual classifiers.
      - **How it helps**: Ensemble methods combine the predictions of multiple classifiers, which can improve overall performance and robustness against class imbalance.


# Lecture 5 
## Logistic Regression
- Logistic regression is a statistical method used for binary classification problems. It models the relationship between a binary dependent variable and one or more independent variables by estimating the probability of the dependent variable being in a particular class.
- It is a type of regression analysis that uses the logistic function to model the probability of a binary outcome.
- Logistic regression is widely used in various fields, including social sciences, healthcare, marketing, and finance, for tasks such as predicting customer churn, disease diagnosis, and credit risk assessment.
- Logistic regression is a linear model that uses the logistic function to map the linear combination of input features to a probability value between 0 and 1.
- The logistic function is defined as:
  - $$f(x) = \frac{1}{1 + e^{-x}}$$ 
  - Where:
    - x: The linear combination of input features and their corresponding weights.
    - e: The base of the natural logarithm (approximately 2.71828).
    - f(x): The output probability value between 0 and 1.
    - This is known as the sigmoid function.
- The logistic regression model can be represented as:
- $$P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n)}}$$
- Where:
  - P(Y=1|X): The probability of the dependent variable Y being equal to 1 given the input features X.
  - β0: The intercept term (bias).
  - β1, β2, ..., βn: The coefficients (weights) for each input feature X1, X2, ..., Xn.
- The coefficients are learned during the training process using maximum likelihood estimation (MLE), which finds the values of the coefficients that maximize the likelihood of the observed data given the model.
- The bias teram helps the model to fit the data better by allowing it to shift the decision boundary. A bias <0 means the model is more likely to predict class 0, while a bias >0 means the model is more likely to predict class 1.

**Log-Odds**:

- The log-odds (or logit) is the logarithm of the odds ratio, which is the ratio of the probability of an event occurring to the probability of it not occurring.
- The log-odds can be calculated as:
  - $$\text{log-odds} = \log\left(\frac{P(Y=1|X)}{1 - P(Y=1|X)}\right) = = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n$$
  
- The log-odds transformation is useful because it maps the probability values (0, 1) to the entire real line (-∞, +∞), allowing for a linear relationship between the input features and the log-odds.
- The log-odds can be interpreted as the change in the log-odds of the dependent variable for a one-unit increase in the independent variable.
- Logg odds of the probability of the positive class can be modelled as a linear function of the input features


### Training Logistic Regression
- **Maximum Likelihood Estimation (MLE)** is used to estimate the coefficients of the logistic regression model.
  - The MLE finds the values of the coefficients that maximize the likelihood of the observed data given the model.
- **L2 regularization (Ridge)** is often used to prevent overfitting by adding a penalty term to the loss function.
  - The regularization term helps to constrain the coefficients, reducing their variance and improving the model's generalization to unseen data.
  - The choice of regularization strength is crucial, as too much regularization can lead to underfitting, while too little can result in overfitting.
  - Cross-validation is commonly used to select the optimal regularization parameter.
  - It is important to evaluate the model's performance on a validation set to ensure that it generalizes well to new data.
- **L1 regularization (Lasso)** can also be used to perform feature selection by driving some coefficients to zero, effectively removing those features from the model.
- **Elastic Net** is a combination of L1 and L2 regularization, allowing for both feature selection and coefficient shrinkage.
- **Bayesian logistic regression** is another approach that incorporates prior distributions on the coefficients, allowing for uncertainty quantification and regularization.


Scikit-Learn's LogisticRegression

- the most commonly used solver is L2-regularized MLE with the `liblinear` solver.
- The available optimization solvers are:
  - `liblinear`: A coordinate descent algorithm for L1 and L2 regularization. Better for small datasets of size < 10,000.
  - `saga`: A stochastic average gradient descent algorithm for L1 and L2 regularization. Suitable for large datasets of size > 10,000.
  - `newton-cg`: A Newton's method algorithm for L2 regularization.
  - `lbfgs`: An optimization algorithm based on the limited-memory Broyden-Fletcher-Goldfarb-Shanno (BFGS) method for L2 regularization. Suitable for small to medium-sized datasets


### Bag of Words vs. TF-IDF
- **Bag of Words (BoW)**: A simple representation of text data where each document is represented as a vector of word counts. It ignores the order of words and focuses on the frequency of each word in the document.
  - Pros: Simple to implement, easy to interpret, works well for many applications.
  - Cons: Ignores word order, can lead to high-dimensional sparse vectors, may not capture semantic meaning.
- **Term Frequency-Inverse Document Frequency (TF-IDF)**: A more sophisticated representation of text data that considers both the frequency of words in a document and their importance across the entire corpus. It assigns higher weights to words that are frequent in a document but rare in the corpus.
- TF-IDF is calculated as:
- $$\text{TF-IDF}(w, d) = \text{TF}(w, d) \cdot \text{IDF}(w)$$
    - TF(w, d): The term frequency of word w in document d.
    - IDF(w): The inverse document frequency of word w, calculated as:
      - $$\text{IDF}(w) = \log\left(\frac{N}{DF(w)}\right)$$
      - Where:
        - N: The total number of documents in the corpus.
        - DF(w): The number of documents containing word w.
        - In this context documents is the number of tokens in the corpus.
  - TF-IDF captures the importance of words in a document relative to the entire corpus, making it more effective for tasks such as text classification and information retrieval.
  - **Pros**: Captures word importance, reduces the impact of common words, better for semantic understanding.
  - **Cons**: More complex to implement, may still lead to high-dimensional sparse vectors.
- **Example**: In a corpus of 100 documents, the word "the" appears in 90 documents, while the word "NLP" appears in 10 documents. The TF-IDF score for "the" will be lower than that for "NLP" because "the" is common across many documents, while "NLP" is more specific to certain documents.

Make table of columns: Feature | BoW | TF-IDF

| Feature | Bag of Words (BoW) | TF-IDF |
|---------|---------------|------------|
| Concept | Counts word frequencies | Weights word importance |
| Dimensionality | High-dimensional sparse vectors | High-dimensional sparse vectors, but weights may reduce freature noise|
| Complexity | Simple to implement | More complex to implement, due to IDF calculation |
| Word Importance | Ignores word importance | Considers word importance relative to the corpus |
| Context Sensitivity | Ignores word order and context | Ignores word order, but captures context through IDF |
| Peformance | Works well for many applications | Better for semantic understanding and information retrieval |
| Use Cases | Text classification, sentiment analysis | Text classification, information retrieval, search engines |
| Limitations | Ignores word order, may lead to high-dimensional vectors | May still lead to high-dimensional vectors, sensitive to document length, Does not capture word order |

### LR Model Assumptions
- **Linearity**: The relationship between the independent variables and the log-odds of the dependent variable is linear. This means that a one-unit change in an independent variable will result in a constant change in the log-odds of the dependent variable.
- **Independence**: The observations are independent of each other. This means that the outcome of one observation does not influence the outcome of another observation.
- **No Multicollinearity**: The independent variables are not highly correlated with each other. This means that the model should not include highly correlated features, as this can lead to unstable coefficient estimates and difficulty in interpreting the results.
- **No Outliers**: The model assumes that there are no extreme outliers in the data that could disproportionately influence the results. Outliers can affect the estimates of the coefficients and the overall fit of the model.
  

### LR Limitations
- **Linearity Assumption**: Logistic regression assumes a linear relationship between the independent variables and the log-odds of the dependent variable. This may not hold true for all datasets, leading to poor performance.
- **Feature Independence**: Logistic regression assumes that the independent variables are independent of each other. In practice, this assumption may not hold, leading to multicollinearity issues.
- **Sensitivity to Outliers**: Logistic regression can be sensitive to outliers, which can disproportionately influence the model's coefficients and predictions.
- **Inability to Capture Complex Relationships**: Logistic regression is a linear model and may not capture complex relationships between features and the target variable. Non-linear relationships may require more advanced models, such as decision trees or neural networks.
- -**Requires Large Sample Size**: Logistic regression requires a sufficient amount of data to produce reliable estimates. Small sample sizes may lead to overfitting and unreliable predictions. Sample size should be at least 10 times the number of features. eg if you have 5 features, you should have at least 50 samples.