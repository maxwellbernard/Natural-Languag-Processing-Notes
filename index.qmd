---
title: "NLP Class Notes"
author: "Maxwell Bernard"
output-dir: docs
format:
    html:
        toc: true
        toc-depth: 3
        toc-expand: true
        css: styles.css
        code-block-line-numbers: true
        code-block-wrap: true
        code-overflow: wrap
        code-output-overflow: wrap
        theme: default
        code-block-theme: default
        highlight-style: pygments
        self-contained: true
---

# Lecture 2

```{python}
# | echo: false
# | label: Library imports
# | output: false
from nltk.corpus import stopwords  # for removing stopwords
from nltk.tokenize import word_tokenize, sent_tokenize, TreebankWordTokenizer  # for tokenization
import spacy  # for spaCy library
from spacy import displacy  # for visualizing named entities
import nltk
from nltk import FreqDist, ne_chunk

# nltk.download("stopwords")  # download stopwords to local machine
from urllib import request  # for fetching data from URLs
import string  # for string operations
from nltk.stem import WordNetLemmatizer, SnowballStemmer  # for lemmatization and stemming
from nltk.tag import pos_tag  # for POS tagging
from nltk.tree import Tree  # for representing syntactic structure
from prettytable import PrettyTable  # for generating ASCII tables
from nltk.util import ngrams  # ngrams is a function that returns the n-grams of a text
from collections import Counter  # Counter is a dict subclass for counting hashable objects
from nltk.lm import MLE  # Maximum Likelihood Estimation
import re  # Regular Expressions
```

## Libraries

|  | NLTK | spaCy |
|---------|---------|---------|
| Methods | string processing library | object-oriented approach - it parses a text, returns document object whose words and sentences are objects themselves | 
| Tokenization | uses regular expression-based methods which are not always accurate | uses a rule-based approach to tokenization which is more accurate |
| POS Tagging | provides wide range of POS taggers (ranging from rule-based to machine learning-based) | uses a deep learning-based POS tagger which is more accurate (also offers pre-trained models in multiple languages) |
| Named Entity Recognition (NER) | provides multiple NER taggers (ranging from rule-based to machine learning-based) | uses a highly efficient deep learning-based NER tagger for detecting entities such as names, organizations, locations, etc. |
| Performance | slower than spaCy | faster than NLTK (due to its optimized implementation in Cython) |

- `Textblob` is another popular library for NLP in Python. It is built on top of NLTK and provides a simple API for common NLP tasks such as tokenization, POS tagging, and sentiment analysis. 
  - It is **not good for large scale production use**

## Normalization

Normalization is the process of converting a text into a standard form. This involves removing any characters that are not part of the standard English alphabet, converting all characters to lowercase, and removing any extra spaces.

Tasks involved in normalization include

- **Tokenization**: Breaking a text into words, phrases, symbols, or other meaningful elements.
- **Case Folding**: Converting all characters to lowercase.
- **Removing Punctuation**: Removing any non-alphanumeric characters.
- **Removing Stopwords**: Removing common words that do not carry much meaning.
- **Stemming or Lemmatization**: Reducing words to their base or root form.
- **Removing Extra Spaces**: Removing any extra spaces between words.
- **Expanding Contractions**: Expanding contractions such as "don't" to "do not".
- **Removing URLs and Emails**: Removing URLs, email addresses, and other web-related content.
- **Removing HTML Tags**: Removing HTML tags from web pages.
- **Removing Emojis and Special Characters**: Removing emojis, emoticons, and other special characters.

**Removal of Stopwords in Python using NLTK**
```{python}
text = "NLTK and spaCy are popular NLP libraries used for text processing."
tokens = word_tokenize(text)
stop_words = set(stopwords.words("english"))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
print(f"NLTK stopwords removal: {' '.join(filtered_tokens)}")
```

**Removal of Stopwords in Python using spaCy**
```{python}
nlp = spacy.load("en_core_web_sm") # Load English language model
text = "NLTK and spaCy are popular NLP libraries used for text processing."
doc = nlp(text)
filtered_tokens = [token.text for token in doc if not token.is_stop]
print(f"spaCy stopwords removal: {' '.join(filtered_tokens)}")
```

#### Morphological Normalization

- Morphological normalization is the process of reducing a word to its base or root form
- This can involve **stemming** or **lemmatization**
- **Roots** are the base forms of words eg. "run" is the root of "running", "ran", "runs"
- **Affixes** are the prefixes and suffixes that are added to roots to create different forms of words eg. "ing", "ed", "s"
  - **Prefixes** are added to the beginning of a word eg. "un" in "undo"
  - **Suffixes** are added to the end of a word eg. "ly" in "quickly"
- **Inflectional** affixes are added to a word to change its grammatical form eg. "s" for plural nouns, "ed" for past tense verbs eg. "dogs", "walked"

### Tokenization

- Tokenization is the process of breaking a text into words, phrases, symbols, or other meaningful elements.
- The tokens are the words, sentences, characters, or subwords that are produced by the tokenization process.
- Space based token is used to prepare the text for further processing such as stemming, lemmatization, and POS tagging.
- **Type** is number of unique tokens in a text
- **Token** is a single instance of a token in a text

```{python}
# | eval: false
tokens = word_tokenize(text)
total_tokens = len(tokens)
total_types = len(set(tokens))
```
- **Corpus** is a collection of text documents

##### Tokenization of raw text in Python
- word.isalnum() - returns True if all characters in the word are alphanumeric

```{python}
#| eval: false
shakespeare_url = "https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt"
crime_punishment_url = "http://www.gutenberg.org/files/2554/2554-0.txt"

# Tokenize Raw Text

shakespeare_text = request.urlopen(shakespeare_url).read().decode("utf8")
crime_punishment_text = request.urlopen(crime_punishment_url).read().decode("utf8")

shakespeare_tokenized = [word for word in word_tokenize(shakespeare_text.lower()) if word.isalnum()]
crime_punishment_tokenized = [
    word for word in word_tokenize(crime_punishment_text.lower()) if word.isalnum()
]

# FreqDist (from nltk) to produce a frequency distribution (listing top 20 most common words)

shakespeare_freq = FreqDist(shakespeare_tokenized)
crime_freq = FreqDist(crime_punishment_tokenized)

shakespeare_common = shakespeare_freq.most_common(20)
crime_common = crime_freq.most_common(20)

print("Top 20 words in Shakespeare")
print(shakespeare_common)

print("\nTop 20 words in Crime and Punishment:")
print(crime_common)
```

##### NLTK Tokenizers
- `word_tokenize()`: Tokenizes a text into words using a regular expression-based tokenizer. A versatile tokenizer that handles contractions, abbreviations, and punctuation marks effectively. It is suitable for most general-purpose tokenization tasks.
- `WordPunctTokenizer()`: Tokenizes a text into words using a regular expression that matches punctuation as separate tokens. A specialized tokenizer that separates words and punctuation explicitly. It is useful when you need to distinguish between alphabetic and non-alphabetic characters.
  
### Stemming

- Stemming is the process of reducing a word to its root or base form. For example, the word "running" would be reduced to "run" by a stemming algorithm.
- Stemmers remove word suffixes by running input word tokens against a pre-defined list of common suffixes.
- Stemming is a heuristic process that does not always produce a valid root word, but it can be useful for text processing tasks such as search indexing and information retrieval.
- **Porter Stemmer** is a popular (rule-based) stemming algorithm that is widely used in text processing applications.
- **Snowball Stemmer** is an improved version of the Porter Stemmer that supports multiple languages.

#### Example of stemming in Python using NLTK

```{python}
sbs = SnowballStemmer("english")
text="The stars twinkled in the dark, illuminating the night sky."
method = TreebankWordTokenizer()
word_tokens = method.tokenize(text)
stemmed = [sbs.stem(token) for token in word_tokens]

for i in range(len(word_tokens)):
    print(f"{word_tokens[i]} | {stemmed[i]}")
```

### Lemmatization

- The process of reducing a word to its base or root form, known as a lemma.
- Is more sophisticated than stemming because it uses a dictionary to map words to their base forms.
- POS tagging is used to determine the correct lemma for a word based on its part of speech.
- Lemmatization is more accurate than stemming but can be slower and more computationally intensive.
- **WordNet Lemmatizer** is a popular lemmatization algorithm that is available in NLTK.

#### Example of stemming in Python using NLTK
```{python}
sbs = SnowballStemmer("english")
text="The stars twinkled in the dark, illuminating the night sky."
method = TreebankWordTokenizer()
word_tokens = method.tokenize(text)
lemma = WordNetLemmatizer()
lemmatized = [lemma.lemmatize(token) for token in word_tokens]

for i in range(len(word_tokens)):
    print(f"{word_tokens[i]} | {lemmatized[i]}")
```


**Comparison table of stemming and lemmatization**

| Stemming | Lemmatization |
|----------|---------------|
| Faster | Slower (Resource Intensive) |
| Less accurate | More accurate |
| Uses heuristics (eg choppping off endings) | Uses a dictionary-based lookup |
| Produces root words | Produces base words |
| Removes word suffixes | Maps words to base forms |
| Does not require POS tagging | Requires POS tagging |


## Regular Expressions

- Regular expressions are a powerful tool for **pattern matching** and text processing.
- They allow you to search for patterns in text, extract specific information, and perform complex text transformations.

**Regular Expression (Disjunction) Table**

| Pattern | Matches | Example |
|---------|---------|---------|
| . | Any character except newline | "a.b" matches "axb", "a1b", "a\@b" |
| ^ | Start of string | "^abc" matches "abc", "abcd", "abc123" |
| \$ | End of string | "abc\$" matches "abc", "123abc", "xyzabc" |
| \* | Zero or more occurrences | "ab*c" matches "ac", "abc", "abbc" |
| \+ | One or more occurrences | "ab+c" matches "abc", "abbc", "abbbc" |
| ? | Zero or one occurrence | "ab?c" matches "ac", "abc" |
| {n} | Exactly n occurrences | "ab{2}c" matches "abbc" |
| {n,} | n or more occurrences | "ab{2,}c" matches "abbc", "abbbc" |
| {n,m} | Between n and m occurrences | "ab{2,3}c" matches "abbc", "abbbc" |
| [abc] | Any character in the set | "[abc]" matches "a", "b", "c" |
| [^abc] | Any character not in the set | "[^abc]" matches "d", "e", "f" |
| [A-Z] | Any character in the range | "[A-Z]" matches "A", "B", "C" |
| [a-z] | Any character in the range | "[a-z]" matches "a", "b", "c" |
| [0-9] | Any digit | "[0-9]" matches "0", "1", "2" |
| \\d | Any digit | "\\d" matches "0", "1", "2" |
| \\D | Any non-digit | "\\D" matches "a", "b", "c" |
| \\w | Any word character | "\\w" matches "a", "b", "c", "0", "1", "2" |
| \\W | Any non-word character | "\\W" matches "@", "#", "$" |
| \\s | Any whitespace character | "\\s" matches " ", "\t", "\n" |
| \\S | Any non-whitespace character | "\\S" matches "a", "b", "c" |

**Regular Expression Method Table**

| Method | Description |
|--------|-------------|
| re.match() | Matches a pattern at the beginning of a string |
| re.search() | Searches for a pattern in a string |
| re.findall() | Finds all occurrences of a pattern in a string |
| re.split() | Splits a string based on a pattern |
| re.sub() | Replaces a pattern in a string with another pattern |


**Example of Regular Expressions in Python**

```{python}
text = "The quick brown fox jumps over the lazy dog. The dog barks at the fox."
sentences = sent_tokenize(text)
words = word_tokenize(text)

# Find all words that start with "b"
pattern = r"\b[bB]\w+"  # \b is a word boundary, \w+ is one or more word characters
for word in words:
    if re.match(pattern, word):
        print(f" All words that start with 'b': {word}")

# Find all sentences that contain the word "dog"
pattern = r"\b[dD]ogs?\b"  # \b is a word boundary, ? is zero or one occurrence of previous character, s? matches "dog" or "dogs"
for sentence in sentences:
    if re.search(pattern, sentence):
        print(f" All sentences containing word 'dog': {sentence}")
```

## POS Tagging

- Part-of-speech (POS) tagging is the process of assigning a part of speech to each word in a text. The part of speech indicates the grammatical category of the word, such as noun, verb, adjective, etc.
- The goal of POS tagging is to identify the syntactic structure of a sentence and extract useful information about the text.

- **Types of POS Tagging**
  - **Rule-based POS Tagging**:
    - Uses hand-crafted rules to assign POS tags to words based on their context.
    - Relies on a predefined set of rules and patterns to determine the correct POS tag for a word.
    - **Pros**:
        - Simple and easy to implement, but may not be as accurate as other methods.
        - Doesnt require a lot of computational resources or training data.
        - Can be easily customized and adapted to different languages and domains.
      - **Cons**:
        - May not be as accurate as other methods, especially for complex or ambiguous cases.
        - Requires manual effort to define rules and patterns for different languages and domains.
        - Limited to the rules and patterns defined by the developer, which may not cover all cases.
      - **Example**: Rule-based POS taggers in NLTK such as the `DefaultTagger` and `RegexpTagger` and `pos_tag` method.
<br>
<br>

  - **Statistical POS Tagging**:
  - Uses statistical models (such as Hidden Markov Models) to assign POS tags to words based on their context and probability.
  - Learns the patterns and relationships between words and their POS tags from a large corpus of annotated text.
    - **Pros**:
      - More accurate than rule-based methods, especially for complex or ambiguous cases.
      - Can handle a wide range of languages and domains without manual intervention.
      - Can be trained on large datasets to improve accuracy and performance.
    - **Cons**:
      - Requires a large amount of annotated training data to train the statistical model.
      - Can be computationally intensive and require significant resources for training and inference.
      - May not be as interpretable or transparent as rule-based methods, making it difficult to understand the model's decisions.
    - **Examples**: Machine learning-based POS taggers in spaCy such as the `PerceptronTagger` and `CNNTagger`.

**Most Common POS Tags (NLTK)**
  
| Tag | Description | Example |
|-----|-------------|---------|
| CC | Coordinating conjunction | and, but, or |
| CD | Cardinal number | 1, 2, 3 |
| DT | Determiner | the, a, an |
| EX | Existential there | there |
| FW | Foreign word | bonjour |
| IN | Preposition or subordinating conjunction | in, of, on |
| JJ | Adjective | big, green, happy |
| JJR | Adjective, comparative | bigger, greener, happier |
| JJS | Adjective, superlative | biggest, greenest, happiest |
| LS | List item marker | 1, 2, 3 |
| MD | Modal | can, could, might |
| NN | Noun, singular or mass | dog, cat, house |
      
<br>
<br>

**Most Common POS Tags (SpaCy)**

| Tag | Description | Example |
|-----|-------------|---------|
| ADJ | Adjective | big, green, happy |
| ADP | Adposition | in, to, during |
| ADV | Adverb | very, tomorrow, down |
| AUX | Auxiliary | is, has (done), will |
| CONJ | Conjunction | and, or, but |
| CCONJ | Coordinating conjunction | and, or, but |
| DET | Determiner | a, an, the |
| INTJ | Interjection | psst, ouch, bravo |
| NOUN | N
| NUM | Numeral | 42, forty-two |

#### Example of POS Tagging in Python using NLTK
```{python}
text = "The quick brown fox jumps over the lazy dog."
words = word_tokenize(text)
tags = pos_tag(words)

for word, tag in tags:
    print(f"{word} | {tag}")
```

#### Example of POS Tagging in Python using spaCy
- Make sure to download the spaCy model using `python -m spacy download en_core_web_sm`
- This model is a small English model trained on written web text (blogs, news, comments), which includes vocabulary, vectors, syntax, and entities.
- The model is trained on the OntoNotes 5 corpus and supports POS tagging, dependency parsing, named entity recognition, and more.
```{python}
model = spacy.load("en_core_web_sm")
sample_text = "The quick brown fox jumps over the lazy dog."
doc = model(sample_text)

for word in doc:
    print(f"{word.text} | {word.pos_}")
```

## Named Entity Recognition (NER)
- Named Entity Recognition (NER) is the process of identifying and classifying named entities in a text.
- Named entities are real-world objects such as persons, organizations, locations, dates, and more.
- NER is an important task in NLP because it helps extract useful information from unstructured text and enables downstream tasks such as information retrieval, question answering, and text summarization.
- NER models are trained on annotated datasets that contain labeled examples of named entities in text.
- **Common types of named entities**:
  - **Person**: Names of people, such as "John Doe" or "Alice Smith".
  - **Organization**: Names of companies, institutions, or groups, such as "Google" or "United Nations".
  - **Location**: Names of places, such as "New York" or "Mount Everest".
  - **Date**: Dates and times, such as "January 1, 2022" or "10:30 AM".
  - **Number**: Numerical quantities, such as "100" or "3.14".
  - **Miscellaneous**: Other named entities, such as "Apple" (the company) or "Python" (the programming language).
  - **Event**: Names of events, such as "World War II" or "Super Bowl".
  - **Product**: Names of products, such as "iPhone" or "Coca-Cola".

#### Example of Named Entity Recognition in Python using NLTK
In NLP, a tree structure is often used to represent the syntactic structure of a sentence. Each node in the tree represents a linguistic unit, such as a word or a phrase, and the edges represent the relationships between these units. The `Tree` class provides various methods for creating, traversing, and modifying these tree structures.
```{python}
text = "Apple is a technology company based in Cupertino, California."
words = word_tokenize(text)
tags = pos_tag(words)
tree = ne_chunk(tags)

for subtree in tree:
    if isinstance(subtree, Tree):
        label = subtree.label()
        words = " ".join([word for word, tag in subtree.leaves()])
        print(f"{label}: {words}")
```
- Note: NLTK's named entity recognizer has identified "Apple", "Cupertino", and "California" as geopolitical entities (GPE).
- The NER model uses context and patterns learned from training data to classify named entities, but it is not always perfect and can sometimes make mistakes, as seen with "Apple" in this case.

#### Example of Named Entity Recognition in Python using spaCy
```{python}
model = spacy.load("en_core_web_sm")
sample_text = "Apple is a technology company based in Cupertino, California."
doc = model(sample_text)

displacy.render(doc, style="ent", jupyter=True) # style="ent" is used to display named entities
```


# Lecture 3

## N-Gram (Probabilistic LM)
This is model predicts the probability of a word given the previous n-1 words. It is based on the assumption that the probability of a word depends on the context provided by the previous n-1 words. N-grams are used in various NLP tasks such as speech recognition, machine translation, and text generation.

- **Unigram**: A single word
- **Bigram**: A pair of words
- **Trigram**: A triplet of words ect...

number of n-grams = (total number of words) - (n - 1)

**Generating different n-grams for a text**:

```{python}
text = "Italy is famous for its cuisine."

def generate_ngrams(text, n):
    tokens = word_tokenize(text)
    ngrams_list = list(ngrams(tokens, n))
    return [" ".join(ngram) for ngram in ngrams_list]

trigrams = generate_ngrams(text, 3)
```

### Conditional Probability and Chain Rule

- **Joint Probability**: The probability of two or more events occurring together. It is denoted as P(A, B) and is calculated as the probability of both events A and B occurring.
  - **Joint Probability Formula**: $P(A \cap B) = P(A) \times P(B)$
  - is equivalent to $P(A \cap B) = P(A) \times P(B)$
<br>
<br>

- **Conditional Probability**: The probability of an event given that another event has occurred. It is denoted as P(A | B) which is the probability of event A occurring given that event B has occurred.
- It is calculated as the probability of both events A and B occurring divided by the probability of event B occurring.
  - **Conditional Probability Formula**: $P(A \mid B) = \frac{P(A, B)}{P(B)}$
<br>
<br>

- **Chain Rule of Probability**: The probability of a sequence of events can be calculated by multiplying the conditional probabilities of each event given the previous events in the sequence.
  - **Chain Rule Formula**:
$$
\small
P(w_1, w_2, ..., w_n) = P(w_1) * P(w_2 \mid w_1) * P(w_3 \mid w_1, w_2) * ... * P(w_n \mid w_1, w_2, ..., w_{n-1})
$$

  - **Example**: The probability of the sequence "The quick brown fox" can be calculated using the chain rule as follows:
$$
\small
\begin{aligned}
P(\text{The quick brown fox}) &= P(\text{The}) \times P(\text{quick} \mid \text{The}) \\
&\times P(\text{brown} \mid \text{The quick}) \\
&\times P(\text{fox} \mid \text{The quick brown})
\end{aligned}
$$
    - "What is the probability of the 10th word given the previous 9 words?"
  - **Note**: The chain rule is used to calculate the probability of a sequence of words in an N-gram model.

### Markov Assumption
To make things manageable, N-gram models approximate the chain rule by making a simplifying assumption called the Markov assumption. This assumption states that the **probability of a word depends only on the previous n-1 words, not on all the previous words in the sequence.** This simplifies the calculation of the conditional probability of a word given the previous n-1 words.


- **First-Order Markov Model**: The probability of a word depends only on the previous word.
  - $$P(w_n | w_1, w_2, ..., w_{n-1}) = P(w_n | w_{n-1})$$
  - "The probability of the 10th word depends only on the 9th word, not on the previous words."
  - **Example**: $P(\text{fox} \mid \text{The quick brown}) = P(\text{fox} \mid \text{brown})$
  - **Note**: The first-order Markov model is a special case of the N-gram model where n=2.
<br>
<br>

- **Second-Order Markov Model**: The probability of a word depends only on the previous two words.
  - $$P(w_n | w_1, w_2, ..., w_{n-1}) = P(w_n | w_{n-2}, w_{n-1})$$
  - "The probability of the 10th word depends only on the 8th and 9th words, not on the previous words."
  - **Example**: $P(\text{fox} \mid \text{The quick brown}) = P(\text{fox} \mid \text{quick brown})$
  - **Note**: The second-order Markov model is a special case of the N-gram model where n=3.

**N-Gram Probability Calculation**

- Mathematically, for a sequence of words $w_1, w_2, ..., w_n$, the probability of the sequence can be calculated using the chain rule of probability:
  - $$\small P(w_1, w_2, ..., w_n) = P(w_1) * P(w_2 \mid w_1) * P(w_3 \mid w_1, w_2) * ... * P(w_n \mid w_1, w_2, ..., w_{n-1})$$

- The probability of a word given the previous n-1 words can be calculated using the formula:
  - $P(w_n | w_1, w_2, ..., w_{n-1}) = \frac{P(w_1, w_2, ..., w_n)}{P(w_1, w_2, ..., w_{n-1})}$
<br>

- The probability of a sequence of words can be calculated by counting the occurrences of the n-gram in a corpus and dividing by the total number of n-grams in the corpus.
<br>

- **The Maximum Likelihood Estimation (MLE) of an N-gram** is calculated as:
  - $P(w_n | w_1, w_2, ..., w_{n-1}) = \frac{\text{Count(n-gram)}}{\text{Count(of previous n-1 words)}}$
<br>
<br>
  - Issue: The MLE can assign zero probability to unseen n-grams, leading to sparsity and poor generalization.
  
### Padding 
Is a technique used to handle the boundary conditions in N-gram models where the context words are not available.
- **Start-of-Sentence (BOS) Padding**: A special token that represents the beginning of a sentence. It is used to handle the first word in a sentence where the context words are not available.
  - **Example**: `<s>` The quick brown fox jumps over the lazy dog. Note `<s>` is equivalent to `<start>`
<br>
<br>

- **End-of-Sentence (EOS) Padding**: A special token that represents the end of a sentence. It is used to handle the last word in a sentence where the context words are not available.
  - **Example**: The fox jumps over the lazy dog. `</s>`

**Notes**: the choice of how many padding tokens to use depends on the order of the N-gram model. For a bigram model, you would use one padding token at the beginning of the sentence. For a trigram model, you would use two padding tokens at the beginning of the sentence. The padding tokens are not part of the vocabulary and are used only for modeling purposes.

**Padding Example in Python**

```{python}
text = "I like NLP. NLP is fun. NLP in python is fun."
sentences = sent_tokenize(text)

# Pad each sentence individually with <s> and </s> tokens and tokenize into words
padded_sentences = []
for sentence in sentences:
    words = word_tokenize(sentence)
    padded = ["<s>"] + words + ["</s>"]
    padded_sentences.append(padded)

for sentence in padded_sentences:
    print(sentence)
```


### Issue of Underflow
- **Underflow**: A numerical issue that occurs when multiplying many small probabilities together, leading to a very small probability that may be rounded to zero.
- **Solution**: Use ``log probabilities`` to avoid underflow by converting the product of probabilities to the sum of log probabilities.

### Smoothing Techniques
- This is a technique used to address the issue of zero probabilities in N-gram models. It assigns a small non-zero probability to unseen n-grams to improve the generalization of the model. You "test" these by comparing smoothed probabilities to unsmoothed ones.
<br>
<br>

- **Laplace (Add-One) Smoothing**: A simple smoothing technique that adds one to the count of each n-gram to avoid zero probabilities.
  - **Formula**: $P(w_n | w_1, w_2, ..., w_{n-1}) = \frac{(\text{Count(n-gram)} + 1)}{(\text{Count(of previous n-1 words)} + V)}$
    - **V**: The vocabulary size, which is the total number of unique words in the corpus.
    - **Example**: $$P(\text{fox} \mid \text{quick brown}) = \frac{\text{Count}(\text{quick brown fox}) + 1}{\text{Count}(\text{quick brown}) + V}$$

- **Add-k Smoothing**: A generalization of Laplace smoothing that adds a `smaller constant k` to the count of each n-gram to avoid zero probabilities.
  - **Formula**: $P(w_n | w_1, w_2, ..., w_{n-1}) = \frac{(\text{Count(n-gram)} + k)}{(\text{Count(of previous n-1 words)} + kV)}$
    - **k**: A constant value that determines the amount of smoothing.
    - **Example**: $$P(\text{fox} \mid \text{quick brown}) = \frac{\text{Count}(\text{quick brown fox}) + k}{\text{Count}(\text{quick brown}) + kV}$$
    - **Test**: Tune k and compare probabilities for rare vs. frequent trigrams.

- **Good-Turing Smoothing**: A more sophisticated smoothing technique that estimates the probability of unseen n-grams based on the frequency of seen n-grams.
  - adjusts the counts of n-grams based on the frequency of n-grams with similar counts.
  - **Test**: compute adjusted probabilities for low-frequency n-grams and validate against held-out data.

- **Backoff and Interpolation**: A technique that combines n-gram models of different orders to improve the generalization of the model.
  - **Backoff**: Uses lower-order n-grams when higher-order n-grams have zero probabilities.
  - **Interpolation**: Combines probabilities from different n-gram models using linear interpolation.
  - **Test**: Compare performance of backoff and interpolation on different datasets.

- **Kneser-Ney Smoothing**: A state-of-the-art smoothing technique that estimates the probability of unseen n-grams based on the frequency of n-grams in the context of the n-gram.
  - It considers how often a word appears in a novel context, rather than just how often it appears overall.
  - **Test**: Compare Kneser-Ney smoothing to other smoothing techniques on large datasets.


### Perplexity
- **Perplexity**: A measure of how well a language model predicts a given text. It is the inverse probability of the test set, normalized by the number of words.
- Evaluation Metric: "How well the model predicts the next word in a sequence."

  - **Perplexity Formula**: $PP(W) = P(w_1, w_2, ..., w_N)^{-\frac{1}{N}}$
    - which can be calculated as $PP(W) = \left(\frac{1}{P(w_1, w_2, ..., w_N)}\right)^{\frac{1}{N}}$
    - where:
      - **P(w_1, w_2, ..., w_N)**: Probability of the test set under the language model.
      - **N**: Number of words in the test set.
      - **Example**: If the perplexity of a language model is 100, it means that the model is as confused as if it had to choose uniformly among 100 words for each word in the test set.
    - **N**: The number of words in the test set.
    - **Example**: If the perplexity of a language model is 100, it means that the model is as confused as if it had to choose uniformly among 100 words for each word in the test set.
  - <span style="color:green">Lower </span>Perplexity: <span style="color:green">better</span> language model that predicts the test set more accurately.
  - <span style="color:red">Higher </span>Perplexity: <span style="color:red">worse</span> language model that predicts the test set less accurately.

### Example of Trigram LM in Python
- Steps:
  - Tokenize text
  - Add padding tokens
  - Generate trigrams
  - Count unique trigrams
  - Calculate trigram probabilities (MLE)
  - Calculate perplexity

**STEP 1: tokenize text and add padding tokens**
```{python}
text = "I like NLP. NLP is fun. NLP in python is fun. I like coding in python. NLP is cool."
tokens = sent_tokenize(text)
padded_sentences = []
for token in tokens:
    words = word_tokenize(token)
    padded = ["<s>"] + words + ["</s>"]
    padded_sentences.append(padded)

print("Padded Sentences:")
for sent in padded_sentences:
    print(sent)
```

**STEP 2: Generate trigrams (and bigrams for MLE calculation)**
```{python}
trigrams = []
for sent in padded_sentences:
    sent_trigrams = list(ngrams(sent, 3))
    # sent_trigrams = list(ngrams(sent, 3, pad_left=False, pad_right=False, left_pad_symbol="<s>", right_pad_symbol="</s>"))
    # can use the above line to avoid padding tokens in trigrams, but its less flexible if you need to reuse the padded data.
    trigrams.extend(sent_trigrams)

bigrams = []
for sent in padded_sentences:
    sent_bigrams = list(ngrams(sent, 2))
    bigrams.extend(sent_bigrams)

print("\nTrigrams:")
for trigram in trigrams:
    print(trigram)
```

**STEP 3: Count unique trigrams and bigrams**
```{python}
trigram_counts = Counter(trigrams)
bigrams_counts = Counter(bigrams)
unique_bigrams = len(bigrams_counts)
unique_trigrams = len(trigram_counts)
print("\nUnique Trigrams:", unique_trigrams)
print("Unique Bigrams:", unique_bigrams)

c_tri_tab = PrettyTable(["Index", "Unique Trigram", "Count"])
for i, (trigram, count) in enumerate(trigram_counts.items()):
    c_tri_tab.add_row([i, trigram, count])
print(c_tri_tab)
```

**STEP 4: Calculate trigram probabilities (MLE)**
```{python}
tri_mle = {}
for (w1, w2, w3), count in trigram_counts.items():
    tri_mle[(w1, w2, w3)] = round(count / bigrams_counts[(w1, w2)], 3)

print("\nTrigram MLE:")
tri_mle_tab = PrettyTable(["Word 1", "Word 2", "Word 3", "MLE"])
for (w1, w2, w3), mle in tri_mle.items():
    tri_mle_tab.add_row([w1, w2, w3, mle])
print(tri_mle_tab)
```

**(Calculating MLE with Laplace Smoothing)**
```{python}
V = len(trigram_counts)
k = 1  # Laplace smoothing constant
tri_mle_laplace = {}

for (w1, w2, w3), count in trigram_counts.items():
    tri_mle_laplace[(w1, w2, w3)] = round((count + k) / (bigrams_counts[(w1, w2)] + k * V), 3)

print("\nTrigram MLE with Laplace Smoothing:")
tri_mle_laplace_tab = PrettyTable(["Word 1", "Word 2", "Word 3", "MLE (Laplace)"])
for (w1, w2, w3), mle in tri_mle_laplace.items():
    tri_mle_laplace_tab.add_row([w1, w2, w3, mle])
print(tri_mle_laplace_tab)
```

**STEP 5: Calculate Perplexity**

```{python}
import math

test_trigrams = trigrams  # Reusing the training trigrams for simplicity
# Calculate the sum of log probabilities
log_prob_sum = 0
N = len(test_trigrams)  # Number of trigrams in the test set
for trigram in test_trigrams:
    prob = tri_mle.get(trigram, 0)  # Get MLE probability, default to 0 if unseen
    if prob > 0:  # Avoid log(0)
        log_prob_sum += math.log2(prob) # use log to avoid underflow issues (A numerical issue that occurs when multiplying many small probabilities together, leading to a very small probability that may be rounded to zero)
    else:
        print(f"Warning: Trigram {trigram} has zero probability (unseen in training)")
        log_prob_sum = float("-inf")  # This will lead to infinite perplexity
        break

# Calculate perplexity
if log_prob_sum != float("-inf"):
    avg_log_prob = log_prob_sum / N
    perplexity = 2 ** (-avg_log_prob)
else:
    perplexity = float("inf")

print(f"Number of test trigrams (N): {N}")
print(f"Sum of log probabilities: {log_prob_sum:.3f}")
print(f"Average log probability: {avg_log_prob:.3f}")
print(f"Perplexity: {perplexity:.3f}")

```

### Example of Trigram LM with NLTK ABC Corpus

```{python}
print(type(list))
```
```{python}
import time
from nltk.util import trigrams, bigrams

start_time = time.time()
from nltk.corpus import abc

# Load the ABC corpus
abc_text = abc.raw("rural.txt")

# Step 1 get sentences from corpus
sentences = abc.sents()[0:2000]
print("Number of sentences:", len(sentences))

# Step 2: Tokenize text and add padding tokens
tokens = []
for sentence in sentences:
    padded_sentence = ["<s>"] + [word.lower() for word in sentence] + ["</s>"]
    tokens.extend(padded_sentence)
print("Example tokens:", tokens[:10])

# Step 3: Generate trigrams
trigram_list = list(trigrams(tokens))
bigram_list = list(bigrams(tokens))

print("Example trigrams:", trigram_list[:5])
print("Example bigrams:", bigram_list[:5])

# Step 4: Count unique trigrams
trigram_counts = Counter(trigram_list)
bigram_counts = Counter(bigram_list)
unique_trigrams = len(trigram_counts)
unique_bigrams = len(bigram_counts)
print("Unique Trigrams:", unique_trigrams)
print("Unique Bigrams:", unique_bigrams)

# Step 5: Calculate trigram probabilities (MLE) with Laplace smoothing and k=1
V = len(trigram_counts)
k = 0.01
trigram_mle_laplace = {}
for (w1, w2, w3), count in trigram_counts.items():
    trigram_mle_laplace[(w1, w2, w3)] = (count + k) / (bigram_counts[(w1, w2)] + k * V)


# Function to predict next word based on conditional probability
def predict_next_word(w1, w2, trigram_mle):
    candidates = {w3: prob for (x1, x2, w3), prob in trigram_mle.items() if x1 == w1 and x2 == w2}
    predicted = max(candidates, key=candidates.get) if candidates else None
    prob = candidates.get(predicted, 0.0) if predicted else 0.0
    return predicted, prob


# Test prediction
w1, w2 = "the", "prime"
predicted_word, probability = predict_next_word(w1, w2, trigram_mle_laplace)
print(f"Predicted next word after '{w1} {w2}': {predicted_word}")
print(f"Probability of the next word occurring: {probability:.5f}")

end_time = time.time()
print(f"Execution time: {end_time - start_time:.5f} seconds")
```

### Example of Sentence Generation with Trigram LM

```{python}
import random

start_time = time.time()


def generate_sentence(model, max_length):
    current_bigram = random.choice(list(model.keys()))  # Pick a random starting bigram
    get_text = list(current_bigram)  # Initialize with the two words from the bigram

    for _ in range(max_length - 2):  # Start from the third word
        w_next_prob = model.get(tuple(get_text[-2:]), {})  # Get trigram probabilities
        if not w_next_prob:  # If no next word, break
            break
        w_next = random.choices(list(w_next_prob.keys()), weights=list(w_next_prob.values()))[0]
        get_text.append(w_next)  # Append next word

    return " ".join(get_text)  # Return generated sentence as a string


# Example usage
generated_text = generate_sentence(trigram_mle_laplace, 15)
print(f"Generated sentence: {generated_text}")
end_time = time.time()
print(f"Execution time: {end_time - start_time:.5f} seconds")
```